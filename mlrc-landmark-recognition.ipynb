{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports and Setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Seed for making reproducible experiments\nseed = 61299\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import tensorflow and print the version\nimport tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\nINPUT_WIDTH = 256\nINPUT_HEIGHT = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_resized(im, w=INPUT_WIDTH, h=INPUT_HEIGHT):\n    \"\"\"\n        Fit an image to the designated input dimensions, \n            padding the edges with black pixels.\n        Given: \n            im - PIL Image to resize\n        Return:\n            the resized PIL Image\n    \"\"\"\n    # create empty background and add to background\n    #   (instead of padding)\n    background = Image.new(\"RGB\", (w, h))\n    \n    # get ratios to background dims\n    w_r = im.width / w\n    h_r = im.height / h\n    aspect = im.width / im.height\n    \n    # use largest ratio as the longest edge of background\n    if w_r > h_r:\n        width = w\n        height = int(w / aspect)\n    else:\n        width = int(h * aspect)\n        height = h\n    \n    resized = im.resize((width, height))\n    \n    # add resized image to background, centered\n    background.paste(\n        resized,\n        ((w - width) // 2, \n         (h - height) // 2))\n    return background","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"## get_resized() usage:\n\n# load a local image\npath = '../input/demo-data/eiffel.jpg'\nimg = Image.open(path)\n\n# resize and show\nresized = get_resized(img)\nimg_a = np.asarray(resized)\nplt.imshow(img_a)\nprint(img_a.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Added by Chirag\n#the entire bulk of the data has been imported as you can see in the data section \n\n# General packages\nimport pandas as pd\nimport numpy as np\n\nfrom IPython.display import Image, display\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nBASE_PATH = '../input/landmark-recognition-2020'\n\nTRAIN_DIR = f'{BASE_PATH}/train'\nTEST_DIR = f'{BASE_PATH}/test'\n\nprint('Reading data...')\ntrain = pd.read_csv(f'{BASE_PATH}/train.csv')\nsubmission = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')\nprint('Reading data completed')\n\n#In the below below three cells three landmakrs with roughly 1100 images each have been chsoen to build our base model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#landmark 1 with ID 113209\nlandmark1 = train[train.landmark_id == 113209]\nlandmark1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#landmark 1 with ID 177870\nlandmark2 = train[train.landmark_id == 177870]\nlandmark2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#landmark 1 with ID 194914\nlandmark3 = train[train.landmark_id == 194914]\nlandmark3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#First merge the above threee dataframes \n#Since the original index is retained we shuffle them, drop the index and selecet the columns we require\ntrain = pd.concat([landmark1, landmark2, landmark3]).sample(frac=1).reset_index()[[\"id\",\"landmark_id\"]]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function taken from https://www.kaggle.com/rohitsingh9990/glr-eda-all-you-need-to-know\n#this is jsut for visualziation \nimport PIL\nfrom PIL import Image, ImageDraw\n\n\ndef display_images(images, title=None): \n    f, ax = plt.subplots(5,5, figsize=(18,22))\n    if title:\n        f.suptitle(title, fontsize = 30)\n\n    for i, image_id in enumerate(images):\n        image_path = os.path.join(TRAIN_DIR, f'{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.jpg')\n        image = Image.open(image_path)\n        \n        ax[i//5, i%5].imshow(image) \n        image.close()       \n        ax[i//5, i%5].axis('off')\n\n        landmark_id = train[train.id==image_id.split('.')[0]].landmark_id.values[0]\n        ax[i//5, i%5].set_title(f\"ID: {image_id.split('.')[0]}\\nLandmark_id: {landmark_id}\", fontsize=\"12\")\n\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pick random 25 images from the dataset and print\nsamples = train.sample(25).id.values\ndisplay_images(samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# added by Cihan\n# yet to be tested\n\ndef collect_sample(w=INPUT_WIDTH, h=INPUT_HEIGHT, landmarks):\n    #input of this function is weight, height and a list of landmarks and it returns a numpy array of all\n    # training examples of landmarks at given width and height\n    \n    \n    train = pd.read_csv(f'{BASE_PATH}/train.csv')\n    landmarks_df = train[train.landmark_id == landmarks[0]]\n    for landmark in landmarks:\n        landmarks.append\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the data into a Tensorflow Dataset\n# Added by Saber\n\nBASE_PATH = '../input/landmark-recognition-2020'\n\nTRAIN_DIR = f'{BASE_PATH}/train'\nTEST_DIR = f'{BASE_PATH}/test'\n\ntrain_csv = pd.read_csv(f'{BASE_PATH}/train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adapted from https://cs230.stanford.edu/blog/datapipeline/\nimport os\nimport tensorflow as tf\n\ndef parse_function(filename, label, \n                   img_dim=256):\n    image_string = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n\n    #This will convert to float values in [0, 1]\n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    resized_image = tf.image.resize(image, [img_dim, img_dim])\n    return resized_image, label\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the filenames and labels\npath='/kaggle/input/landmark-recognition-2020/train'\n\nn_samples = 256\nimage_ids = list(train_csv['id'][:n_samples])\nfilenames = [os.path.join(path, f'{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.jpg') for image_id in image_ids]\nlabels = list(train_csv['landmark_id'][:n_samples])\ndel image_ids\nbatch_size = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the Dataset object\nwith tf.device('/cpu:0'):\n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n    dataset = dataset.shuffle(len(filenames))\n    dataset = dataset.map(parse_function, num_parallel_calls=4)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for one_element in dataset:\n#     print(one_element)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Networks Architectures"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\n\n#to train a simple example of a CNN importing MNIST dataset from keras\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\nlabel_binarizer = LabelBinarizer()\n\n#encode the labels to one hot vector\ntrain_labels = label_binarizer.fit_transform(train_labels)\ntest_labels = label_binarizer.transform(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Softmax, Conv2D, MaxPooling2D, AveragePooling2D\n\n#define the sequential neural network\nmodel = Sequential([\n    Conv2D(16,(3,3),padding = 'SAME', activation = 'relu', input_shape = (28,28,1),data_format = 'channels_last'),\n    MaxPooling2D((3,3)),\n    Flatten(input_shape = (28,28)),\n    Dense(16,activation = 'relu'),\n    Dense(10,activation = 'softmax')\n])\n\n#print the summary of the network\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set up compiling options\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nmodel.compile(optimizer = opt,\n             loss = 'categorical_crossentropy',\n             metrics = [mae]\n             )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LeNet-5"},{"metadata":{"trusted":true},"cell_type":"code","source":"leNet5 = Sequential([\n    Conv2D(6,(5,5), activation = 'tanh',padding = 'SAME', input_shape = (28,28,1),data_format = 'channels_last'),\n    AveragePooling2D((2,2)),\n    Conv2D(16,(5,5), activation = 'tanh'),\n    AveragePooling2D((2,2)),\n    Conv2D(120,(5,5), activation = 'tanh'),\n    Flatten(),\n    Dense(84,activation = 'tanh'),\n    Dense(10,activation = 'softmax')\n])\nleNet5.summary()\n\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nleNet5.compile(optimizer = opt,\n             loss = 'categorical_crossentropy',\n             metrics = [mae]\n             )\n\nhistory = leNet5.fit(train_images[...,np.newaxis],train_labels,epochs = 1, batch_size = 256)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AlexNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"alexNet = Sequential([\n    Conv2D(96,(11,11), activation = 'relu',padding = 'VALID', stride = 4, input_shape = (227,227,3), data_format = 'channels_last'),\n    MaxPooling2D((3,3), stride = 2),\n    Conv2D(256,(5,5), padding = 'SAME', activation = 'relu'),\n    MaxPooling2D((3,3)),\n    Conv2D(120,(5,5), activation = 'tanh'),\n    Conv2D(256,(5,5), padding = 'SAME', activation = 'relu'),\n    Conv2D(256,(5,5), padding = 'SAME', activation = 'relu'),\n    Conv2D(256,(5,5), padding = 'SAME', activation = 'relu'),\n    Flatten(),\n    Dense(84,activation = 'tanh'),\n    Dense(10,activation = 'softmax')\n])\nalexNet.summary()\n\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nalexNet.compile(optimizer = opt,\n             loss = 'categorical_crossentropy',\n             metrics = [mae]\n             )\n\nhistory = alexNet.fit(train_images[...,np.newaxis],train_labels,epochs = 1, batch_size = 256)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GoogleLeNet"},{"metadata":{},"cell_type":"markdown","source":"## VGG-16"},{"metadata":{},"cell_type":"markdown","source":"## ResNet"},{"metadata":{},"cell_type":"markdown","source":"## Xception"},{"metadata":{},"cell_type":"markdown","source":"## SENet"},{"metadata":{},"cell_type":"markdown","source":"## Spatial Pyramid Pooling"},{"metadata":{},"cell_type":"markdown","source":"# Training, Diagnosing and Evaulating the Neural Networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\nhistory = model.fit(train_images[...,np.newaxis],train_labels,epochs = 1, batch_size = 256)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}