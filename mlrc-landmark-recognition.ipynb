{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports and Setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"tags":["outputPrepend"]},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Seed for making reproducible experiments\nseed = 61299\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../data'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import tensorflow and print the version\nimport tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\nINPUT_WIDTH = 256\nINPUT_HEIGHT = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_resized(im, w=INPUT_WIDTH, h=INPUT_HEIGHT):\n    \"\"\"\n        Fit an image to the designated input dimensions, \n            padding the edges with black pixels.\n        Given: \n            im - PIL Image to resize\n        Return:\n            the resized PIL Image\n    \"\"\"\n    # create empty background and add to background\n    #   (instead of padding)\n    background = Image.new(\"RGB\", (w, h))\n    \n    # get ratios to background dims\n    w_r = im.width / w\n    h_r = im.height / h\n    aspect = im.width / im.height\n    \n    # use largest ratio as the longest edge of background\n    if w_r > h_r:\n        width = w\n        height = int(w / aspect)\n    else:\n        width = int(h * aspect)\n        height = h\n    \n    resized = im.resize((width, height))\n    \n    # add resized image to background, centered\n    background.paste(\n        resized,\n        ((w - width) // 2, \n         (h - height) // 2))\n    return background","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"## get_resized() usage:\n\n# load a local image\npath = '../input/demo-data/eiffel.jpg'\nimg = Image.open(path)\n\n# resize and show\nresized = get_resized(img)\nimg_a = np.asarray(resized)\nplt.imshow(img_a)\nprint(img_a.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Added by Chirag\n#the entire bulk of the data has been imported as you can see in the data section \n\n# General packages\nimport pandas as pd\nimport numpy as np\n\nfrom IPython.display import Image, display\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nBASE_PATH = '../input/landmark-recognition-2020'\n\nTRAIN_DIR = f'{BASE_PATH}/train'\nTEST_DIR = f'{BASE_PATH}/test'\n\nprint('Reading data...')\ntrain = pd.read_csv(f'{BASE_PATH}/train.csv')\nsubmission = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')\nprint('Reading data completed')\n\n#In the below below three cells three landmakrs with roughly 1100 images each have been chsoen to build our base model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#landmark 1 with ID 113209\nlandmark1 = train[train.landmark_id == 113209]\nlandmark1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#landmark 1 with ID 177870\nlandmark2 = train[train.landmark_id == 177870]\nlandmark2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#landmark 1 with ID 194914\nlandmark3 = train[train.landmark_id == 194914]\nlandmark3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#First merge the above threee dataframes \n#Since the original index is retained we shuffle them, drop the index and selecet the columns we require\ntrain = pd.concat([landmark1, landmark2, landmark3]).sample(frac=1).reset_index()[[\"id\",\"landmark_id\"]]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function taken from https://www.kaggle.com/rohitsingh9990/glr-eda-all-you-need-to-know\n#this is jsut for visualziation \nimport PIL\nfrom PIL import Image, ImageDraw\n\n\ndef display_images(images, title=None): \n    f, ax = plt.subplots(5,5, figsize=(18,22))\n    if title:\n        f.suptitle(title, fontsize = 30)\n\n    for i, image_id in enumerate(images):\n        image_path = os.path.join(TRAIN_DIR, f'{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.jpg')\n        image = Image.open(image_path)\n        \n        ax[i//5, i%5].imshow(image) \n        image.close()       \n        ax[i//5, i%5].axis('off')\n\n        landmark_id = train[train.id==image_id.split('.')[0]].landmark_id.values[0]\n        ax[i//5, i%5].set_title(f\"ID: {image_id.split('.')[0]}\\nLandmark_id: {landmark_id}\", fontsize=\"12\")\n\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pick random 25 images from the dataset and print\nsamples = train.sample(25).id.values\ndisplay_images(samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# added by Cihan\n\nfrom pathlib import Path\n\ndef collect_sample(landmarks, w=INPUT_WIDTH, h=INPUT_HEIGHT):\n    \"\"\"\n        Collect a sample of images determined by a list of\n        landmark ids and resize. Before using the function\n        BASE_PATH and TRAIN_PATH variables must be defined\n        Given: \n            landmarks - a list of landmark ids to be collected\n            w - the width of the images to be stored\n            h - the height of the images to be stored\n        Return:\n            a numpy array with collected samples\n            a pandas series with labels (landmark ids) of each image\n    \"\"\"\n    #read the train csv including image names and corresponding landsmark id\n    train = pd.read_csv(BASE_PATH / 'train.csv')\n\n    #filter the images to be collected and record the size of the sample\n    landmarks_df = train[train.landmark_id.isin(landmarks)]\n    n = landmarks_df.shape[0]\n\n    #create numpy array and pandas data frame to keep images and labels\n    np_imgs = np.zeros((n,w,h,3), dtype = np.uint8)\n    img_labels = pd.Series(['']*100)\n\n    #loop over data frame and record images to the numpy array after resizing\n    for i, landmark in enumerate(landmarks_df.itertuples()):\n        image_path = os.path.join(TRAIN_DIR / f'{landmark.id[0]}/{landmark.id[1]}/{landmark.id[2]}/{landmark.id}.jpg')\n        img = Image.open(image_path)\n        resized_image = get_resized(img, w, h)\n        np_imgs[i,:,:,:] = np.asarray(resized_image)\n        img_labels.at[i] = landmark.id\n    return np_imgs, img_labels   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#added by Cihan\n#An example usage of the collect sample function\nimport matplotlib.pyplot as plt\n\nINPUT_HEIGHT = 256\nINPUT_WIDTH = 256\nBASE_PATH = Path('../data')\nTRAIN_DIR = BASE_PATH / 'train'\nTEST_DIR = BASE_PATH / 'test'\nlandmarks = [113209]\nnp_images, image_labels = collect_sample(landmarks, 256, 256)\nplt.imshow(np_images[10,:,:,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#added by Cihan\n#A short script to copy some of the images to another directory called sample\n# to obtain a relatively small sized data to troubleshot the code\n# without waiting long training times. \n# Other than BASE_PATH, a list of landmarks needs to be provided to be picked\n# by the collecter. Inner directories of sample directory is creted same\n# as the original data directory\nimport os\nimport shutil\n\n#BASE_PATH = Path('../data')\n#TRAIN_DIR = BASE_PATH / 'train'\n#TEST_DIR = BASE_PATH / 'test'\nTARGET_PATH = BASE_PATH / 'sample'\nif not os.path.exists(TARGET_PATH):\n    os.mkdir(TARGET_PATH)\n\nlandmarks = [113209, 177870, 194914]\ntrain = pd.read_csv(BASE_PATH / 'train.csv')\nlandmarks_df = train[train.landmark_id.isin(landmarks)]\nfor i, landmark in enumerate(landmarks_df.itertuples()):\n        image_path = TRAIN_DIR / f'{landmark.id[0]}/{landmark.id[1]}/{landmark.id[2]}/{landmark.id}.jpg'\n        image_target_path = TARGET_PATH / f'{landmark.id[0]}/{landmark.id[1]}/{landmark.id[2]}'\n        if not os.path.exists(image_target_path):\n            os.makedirs(image_target_path)\n        shutil.copy2(image_path, image_target_path)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the data into a Tensorflow Dataset\n# Added by Saber\n\nBASE_PATH = '../input/landmark-recognition-2020'\n\nTRAIN_DIR = f'{BASE_PATH}/train'\nTEST_DIR = f'{BASE_PATH}/test'\n\ntrain_csv = pd.read_csv(f'{BASE_PATH}/train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adapted from https://cs230.stanford.edu/blog/datapipeline/\nimport os\nimport tensorflow as tf\n\ndef parse_function(filename, label, \n                   img_dim=256):\n    image_string = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n\n    #This will convert to float values in [0, 1]\n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    resized_image = tf.image.resize(image, [img_dim, img_dim])\n    return resized_image, label\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the filenames and labels\npath='/kaggle/input/landmark-recognition-2020/train'\n\nn_samples = 256\nimage_ids = list(train_csv['id'][:n_samples])\nfilenames = [os.path.join(path, f'{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.jpg') for image_id in image_ids]\nlabels = list(train_csv['landmark_id'][:n_samples])\ndel image_ids\nbatch_size = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the Dataset object\nwith tf.device('/cpu:0'):\n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n    dataset = dataset.shuffle(len(filenames))\n    dataset = dataset.map(parse_function, num_parallel_calls=4)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for one_element in dataset:\n#     print(one_element)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Networks Architectures"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\n\n#to train a simple example of a CNN importing MNIST dataset from keras\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\nlabel_binarizer = LabelBinarizer()\n\n#encode the labels to one hot vector\ntrain_labels = label_binarizer.fit_transform(train_labels)\ntest_labels = label_binarizer.transform(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Softmax, Conv2D, MaxPooling2D, AveragePooling2D\n\n#define the sequential neural network\nmodel = Sequential([\n    Conv2D(16,(3,3),padding = 'SAME', activation = 'relu', input_shape = (28,28,1),data_format = 'channels_last'),\n    MaxPooling2D((3,3)),\n    Flatten(input_shape = (28,28)),\n    Dense(16,activation = 'relu'),\n    Dense(10,activation = 'softmax')\n])\n\n#print the summary of the network\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set up compiling options\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nmodel.compile(optimizer = opt,\n             loss = 'categorical_crossentropy',\n             metrics = [mae]\n             )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LeNet-5"},{"metadata":{"trusted":true},"cell_type":"code","source":"leNet5 = Sequential([\n    Conv2D(6,(5,5), activation = 'tanh',padding = 'SAME', input_shape = (28,28,1),data_format = 'channels_last'),\n    AveragePooling2D((2,2)),\n    Conv2D(16,(5,5), activation = 'tanh'),\n    AveragePooling2D((2,2)),\n    Conv2D(120,(5,5), activation = 'tanh'),\n    Flatten(),\n    Dense(84,activation = 'tanh'),\n    Dense(10,activation = 'softmax')\n])\nleNet5.summary()\n\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nleNet5.compile(optimizer = opt,\n             loss = 'categorical_crossentropy',\n             metrics = [mae]\n             )\n\nhistory = leNet5.fit(train_images[...,np.newaxis],train_labels,epochs = 1, batch_size = 256)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AlexNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"alexNet = Sequential([\n    Conv2D(96,(11,11), activation = 'relu',padding = 'VALID', stride = 4, input_shape = (227,227,3), data_format = 'channels_last'),\n    MaxPooling2D((3,3), stride = 2),\n    Conv2D(256,(5,5), padding = 'SAME', activation = 'relu'),\n    MaxPooling2D((3,3)),\n    Conv2D(120,(5,5), activation = 'tanh'),\n    Conv2D(256,(5,5), padding = 'SAME', activation = 'relu'),\n    Conv2D(256,(5,5), padding = 'SAME', activation = 'relu'),\n    Conv2D(256,(5,5), padding = 'SAME', activation = 'relu'),\n    Flatten(),\n    Dense(84,activation = 'tanh'),\n    Dense(10,activation = 'softmax')\n])\nalexNet.summary()\n\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nalexNet.compile(optimizer = opt,\n             loss = 'categorical_crossentropy',\n             metrics = [mae]\n             )\n\nhistory = alexNet.fit(train_images[...,np.newaxis],train_labels,epochs = 1, batch_size = 256)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GoogleLeNet"},{"metadata":{},"cell_type":"markdown","source":"## VGG-16"},{"metadata":{},"cell_type":"markdown","source":"## ResNet"},{"metadata":{},"cell_type":"markdown","source":"## Xception"},{"metadata":{},"cell_type":"markdown","source":"## SENet"},{"metadata":{},"cell_type":"markdown","source":"## Spatial Pyramid Pooling"},{"metadata":{},"cell_type":"markdown","source":"# Training, Diagnosing and Evaulating the Neural Networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\nhistory = model.fit(train_images[...,np.newaxis],train_labels,epochs = 1, batch_size = 256)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}